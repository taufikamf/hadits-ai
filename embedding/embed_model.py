import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import json
import pickle
import re
from pathlib import Path
from dotenv import load_dotenv

from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.preprocessing import normalize
from utils.improved_keyword_extractor import EnhancedIslamicKeywordExtractor

# Muat environment variable
load_dotenv()
DATA_PATH = os.getenv("DATA_CLEAN_PATH", "data/processed/hadits_docs.json")
OUTPUT_PATH = "data/processed/hadits_embeddings.pkl"
KEYWORDS_PATH = "data/processed/enhanced_keywords_map.json"  # Updated to use enhanced keywords
MODEL_NAME = "intfloat/e5-small-v2"

# ------------------------------
# âœ… Load model embedding sekali
def get_embedding_model():
    return SentenceTransformer(MODEL_NAME)

# ------------------------------
# 1. Load dokumen
def load_documents():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def load_keyword_map():
    """Load enhanced keywords map generated by EnhancedIslamicKeywordExtractor"""
    if not os.path.exists(KEYWORDS_PATH):
        print(f"[!] Enhanced keywords file not found at {KEYWORDS_PATH}")
        print("[~] Generating enhanced keywords map...")
        # Generate enhanced keywords if not exists
        generate_enhanced_keywords()
    
    with open(KEYWORDS_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
        # Return the keywords part if it's the new format with metadata
        return data.get("keywords", data)

def generate_enhanced_keywords():
    """Generate enhanced keywords map using the improved extractor"""
    print("[~] Loading hadits documents for keyword extraction...")
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        docs = json.load(f)
    
    # Extract texts for keyword extraction
    texts = [doc["terjemah"] for doc in docs if "terjemah" in doc]
    
    # Initialize enhanced extractor
    extractor = EnhancedIslamicKeywordExtractor(min_frequency=15, max_ngram=3)
    
    # Generate enhanced keywords map
    keywords_map = extractor.create_enhanced_keywords_map(texts)
    
    # Save the results
    os.makedirs(os.path.dirname(KEYWORDS_PATH), exist_ok=True)
    result = {
        "metadata": {
            "description": "Enhanced Islamic keywords extracted from hadits collections",
            "extraction_version": "enhanced_v2.0",
            "total_groups": len(keywords_map),
            "extraction_method": "enhanced_islamic_semantic_grouping"
        },
        "keywords": keywords_map
    }
    
    with open(KEYWORDS_PATH, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"[OK] Enhanced keywords saved to {KEYWORDS_PATH}")

def build_semantic_tags(doc, keyword_map):
    """Build semantic tags using enhanced keyword matching"""
    tags = set()
    teks = doc["terjemah"].lower()

    for key, variants in keyword_map.items():
        for v in variants:
            pattern = rf"(?<!\w){re.escape(v.lower())}(?!\w)" 
            if re.search(pattern, teks):
                tags.add(v)

    return ", ".join(sorted(tags))

# 4. Ubah dokumen jadi corpus format "passage: ... Kata kunci penting: ..."
def prepare_corpus(docs, keyword_map):
    corpus = []

    for i, doc in enumerate(docs[:10]):
        print(f"[DEBUG] Doc {i+1} tags: {build_semantic_tags(doc, keyword_map)}")

    for i, doc in enumerate(docs):
        if "id" not in doc:
            raise ValueError(f"[!] Dokumen ke-{i} tidak memiliki ID.")

        base = doc["terjemah"]
        tags = build_semantic_tags(doc, keyword_map)

        if tags:
            full = f"passage: {base}. Kata kunci penting: {tags}"
            if len(tags) <= 1:
                print(f"[WARNING] Doc {doc['id']} hanya memiliki 1 tag: {tags}")
        else:
            full = f"passage: {base}"

        corpus.append(full)

    return corpus

# 5. Embedding dokumen
def embed_documents(corpus, model_name=MODEL_NAME):
    print(f"[~] Embedding {len(corpus)} dokumen...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(corpus, convert_to_numpy=True, normalize_embeddings=True)
    embeddings = normalize(embeddings, axis=1)
    return embeddings

# 6. Simpan hasil embedding + dokumen asli
def save_output(embeddings, documents):
    with open(OUTPUT_PATH, "wb") as f:
        pickle.dump({
            "embeddings": embeddings,
            "documents": documents
        }, f)
    print(f"[OK] Embeddings berhasil disimpan ke {OUTPUT_PATH}")

# ------------------------------
# Main: jalankan semua
if __name__ == "__main__":
    print("[~] Memuat dokumen...")
    docs = load_documents()

    print("[~] Memuat keyword map...")
    keyword_map = load_keyword_map()

    print("[~] Menyusun corpus semantik...")
    corpus = prepare_corpus(docs, keyword_map)

    print("\n[SAMPLE] Contoh Corpus:")
    for i in range(min(3, len(corpus))):
        print(f"[{i+1}] {corpus[i]}")

    print("[~] Proses embedding...")
    embeddings = embed_documents(corpus)

    save_output(embeddings, docs)
