# 5.3.1 Metodologi Evaluasi ROUGE dan BLEU

## Latar Belakang Metodologi

Evaluasi sistem Retrieval-Augmented Generation (RAG) memerlukan metrik yang dapat mengukur kualitas generasi teks secara objektif dan konsisten. Penelitian ini menggunakan dua metrik utama yang telah terbukti efektif dalam evaluasi sistem Natural Language Generation (NLG): ROUGE dan BLEU.

## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

### Definisi dan Konsep Dasar

ROUGE adalah metrik evaluasi yang fokus pada pengukuran recall dengan membandingkan n-gram overlap antara teks yang dihasilkan sistem dengan teks referensi (ground truth). ROUGE dikembangkan khusus untuk evaluasi automatic summarization dan telah diadopsi luas untuk evaluasi berbagai tugas NLG.

### Varian ROUGE yang Digunakan

#### 1. ROUGE-1 (Unigram Overlap)
- **Definisi**: Mengukur overlap unigram (kata tunggal) antara teks yang dihasilkan dengan teks referensi
- **Formula**: 
  ```
  ROUGE-1 = |Unigram_generated ∩ Unigram_reference| / |Unigram_reference|
  ```
- **Interpretasi**: Menunjukkan seberapa banyak kata penting dari referensi yang berhasil ditangkap oleh sistem
- **Kelebihan**: Mudah dipahami dan memberikan gambaran umum coverage konten
- **Kelemahan**: Tidak mempertimbangkan urutan kata atau makna semantik

#### 2. ROUGE-2 (Bigram Overlap)
- **Definisi**: Mengukur overlap bigram (pasangan kata berurutan) antara teks yang dihasilkan dengan teks referensi
- **Formula**: 
  ```
  ROUGE-2 = |Bigram_generated ∩ Bigram_reference| / |Bigram_reference|
  ```
- **Interpretasi**: Memberikan pemahaman yang lebih baik tentang fluency dan struktur frasa
- **Kelebihan**: Mempertimbangkan konteks lokal dan urutan kata
- **Kelemahan**: Lebih sensitif terhadap variasi linguistik

#### 3. ROUGE-L (Longest Common Subsequence)
- **Definisi**: Mengukur Longest Common Subsequence (LCS) antara teks yang dihasilkan dengan teks referensi
- **Formula**: 
  ```
  ROUGE-L = LCS(generated, reference) / length(reference)
  ```
- **Interpretasi**: Menangkap struktur kalimat dan urutan kata tanpa memerlukan n-gram yang berurutan
- **Kelebihan**: Fleksibel terhadap variasi struktur kalimat sambil mempertahankan urutan informasi
- **Kelemahan**: Komputasi lebih kompleks

### Implementasi ROUGE dalam Penelitian

```python
def calculate_rouge_scores(self, reference: str, generated: str) -> Dict[str, float]:
    """
    Menghitung skor ROUGE antara teks referensi dan teks yang dihasilkan.
    
    Args:
        reference (str): Teks referensi (ground truth)
        generated (str): Teks yang dihasilkan sistem
        
    Returns:
        Dict[str, float]: Skor ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)
    """
    try:
        scores = self.rouge.get_scores(generated, reference, avg=True)
        return {
            "rouge-1": scores['rouge-1']['f'],
            "rouge-2": scores['rouge-2']['f'], 
            "rouge-l": scores['rouge-l']['f']
        }
    except Exception as e:
        logger.warning(f"ROUGE calculation failed: {e}")
        return {"rouge-1": 0.0, "rouge-2": 0.0, "rouge-l": 0.0}
```

## BLEU (Bilingual Evaluation Understudy)

### Definisi dan Konsep Dasar

BLEU adalah metrik evaluasi yang awalnya dikembangkan untuk machine translation, namun telah diadaptasi untuk berbagai tugas text generation. BLEU mengukur precision dengan membandingkan n-gram antara teks yang dihasilkan dengan teks referensi, dilengkapi dengan brevity penalty untuk menghindari teks yang terlalu pendek.

### Komponen BLEU Score

#### 1. N-gram Precision
- **Definisi**: Mengukur proporsi n-gram dalam teks yang dihasilkan yang muncul dalam teks referensi
- **Formula untuk n-gram precision**:
  ```
  P_n = (Σ_C∈{Candidates} Σ_n-gram∈C Count_clip(n-gram)) / 
        (Σ_C∈{Candidates} Σ_n-gram∈C Count(n-gram))
  ```
- **Count_clip**: Membatasi count maksimum n-gram sesuai dengan frekuensi dalam referensi

#### 2. Brevity Penalty (BP)
- **Tujuan**: Menghindari bias terhadap teks pendek yang cenderung memiliki precision tinggi
- **Formula**:
  ```
  BP = 1 jika c > r
  BP = e^(1-r/c) jika c ≤ r
  ```
  dimana c = panjang kandidat, r = panjang referensi

#### 3. Final BLEU Score
- **Formula**:
  ```
  BLEU = BP × exp(Σ_{n=1}^N w_n log P_n)
  ```
- **Weights**: w_n adalah bobot untuk n-gram (biasanya uniform: w_n = 1/N)

### Implementasi BLEU dalam Penelitian

```python
def calculate_bleu_score(self, reference: str, generated: str) -> float:
    """
    Menghitung BLEU score antara teks referensi dan teks yang dihasilkan.
    
    Args:
        reference (str): Teks referensi
        generated (str): Teks yang dihasilkan sistem
        
    Returns:
        float: BLEU score (0-1)
    """
    try:
        import nltk
        reference_tokens = nltk.word_tokenize(reference.lower())
        generated_tokens = nltk.word_tokenize(generated.lower())
        
        # Menggunakan smoothing function untuk menangani zero n-gram matches
        smoothing = SmoothingFunction().method1
        
        # Menghitung BLEU score dengan bobot seimbang untuk 1-gram hingga 4-gram
        score = sentence_bleu(
            [reference_tokens], 
            generated_tokens,
            weights=(0.25, 0.25, 0.25, 0.25),
            smoothing_function=smoothing
        )
        return score
    except Exception as e:
        logger.warning(f"BLEU calculation failed: {e}")
        return 0.0
```

## Konfigurasi Evaluasi

### Preprocessing Teks
1. **Tokenisasi**: Menggunakan NLTK word tokenizer untuk konsistensi
2. **Case Normalization**: Konversi ke lowercase untuk mengurangi bias
3. **Punctuation Handling**: Mempertahankan tanda baca yang relevan untuk konteks

### Parameter Evaluasi
- **ROUGE Configuration**: Menggunakan F1-score untuk keseimbangan precision dan recall
- **BLEU Configuration**: 
  - N-gram weights: (0.25, 0.25, 0.25, 0.25) untuk 1-4 gram
  - Smoothing: Method1 untuk menangani corpus kecil
  - Case sensitivity: False

## Validasi Metodologi

### Konsistensi Internal
- **Test-Retest Reliability**: Evaluasi berulang pada dataset yang sama menunjukkan konsistensi hasil
- **Inter-Metric Correlation**: Analisis korelasi antara ROUGE dan BLEU untuk validasi konvergen

### Relevansi Domain
- **Islamic Content Adaptation**: Metodologi disesuaikan untuk konten hadits dengan pertimbangan:
  - Terminologi Arab dan transliterasi
  - Struktur teks hadits yang khas
  - Variasi terjemahan yang valid

## Limitasi Metodologi

### ROUGE Limitations
1. **Lexical Focus**: Hanya mengukur overlap leksikal, tidak semantik
2. **Reference Dependency**: Kualitas evaluasi bergantung pada kualitas referensi
3. **Multiple Valid Outputs**: Tidak mengakomodasi multiple valid answers

### BLEU Limitations
1. **Precision Bias**: Fokus pada precision, kurang mempertimbangkan recall
2. **N-gram Rigidity**: Tidak fleksibel terhadap parafrase yang valid
3. **Short Text Bias**: Meskipun ada brevity penalty, masih bias terhadap teks pendek

## Mitigasi Limitasi

1. **Kombinasi Metrik**: Menggunakan ROUGE dan BLEU secara bersamaan untuk perspektif yang komprehensif
2. **Semantic Similarity**: Menambahkan evaluasi semantic similarity sebagai metrik pelengkap
3. **Domain-Specific Evaluation**: Pertimbangan khusus untuk konten hadits dan konteks Islam
4. **Human Evaluation**: Validasi sampel dengan evaluasi manual (future work)

## Kesimpulan Metodologi

Metodologi evaluasi yang mengombinasikan ROUGE dan BLEU memberikan framework yang robust untuk mengukur kualitas output sistem RAG pada domain hadits. Meskipun memiliki limitasi, kombinasi kedua metrik ini memberikan gambaran yang cukup komprehensif tentang performa sistem dalam hal coverage konten (ROUGE) dan quality generation (BLEU).
